---
title: 'Problem Set #2'
author: '"Insert Name"'
date: "2022-10-06"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

*Note:* Please prepare your answers using Rmarkdown and submit a pdf via Canvas. Each submission has to include all code and R output used to answer the questions. I encourage you to work on the assignments together, but each of you have to type up their responses individually. Identical submissions will not be accepted. Late submissions only receive partial credit.

Since the data set for this assignment is quite large, you'll have to use RStudio on your own computer rather than working directly in RStudio Cloud.

In a widely discussed paper, Michel and colleagues (2011) analyzed the content of more than five million digitized books in an attempt to identify long-term cultural trends. The data that they used has now been released as the Google NGrams dataset, and so we can use the data to replicate and extend some of their work. In one of the many results in the paper, Michel and colleagues argued that we are forgetting faster and faster. For a particular year, say "1883," they calculated the proportion of 1-grams published in each year between 1875 and 1975 that were "1883." They reasoned that this proportion is a measure of the interest in events that happened in that year. In their figure 3a, they plotted the usage trajectories for three years: 1883, 1910, and 1950. These three years share a common pattern: little use before that year, then a spike, then decay. Next, to quantify the rate of decay for each year, Michel and colleagues calculated the “half-life” of each year for all years between 1875 and 1975. In their figure 3a (inset), they showed that the half-life of each year is decreasing, and they argued that this means that we are forgetting the past faster and faster. They used version 1 of the English language corpus, but subsequently Google has released a second version of the corpus. Please read all the parts of the question before you begin coding. This activity will give you practice writing reusable code, interpreting results, and data wrangling (such as working with awkward files and handling missing data). This activity will also help you get up and running with a rich and interesting dataset.

```{r}
# Load the necessary packages.
library(tidyverse)
library(ngramr)
```

a) Get the raw data from the Google Books NGram Viewer website. In particular, you should use version 2 of the English language corpus, which was released on July 1, 2012. Uncompressed, this file is 1.4 GB.

```{r}
## Alternative method without the ngramr package.

# Loading the data into R.
# This stage can of course be written to download the file from the Internet, unzip the .gz file, and load it into R, if you don't want to have the file(s) on your hard drive. However, that should not create any issues with the code below. You only need to write the code relevant to downloading and unzipping prior to the "read_table" commands, after which the code below should still operate as normal.

mydata1 <- read_table("/Users/kaanaksoy/Desktop/UWM/Fall 2022/POL SCI 935/Assignment 2/totalcounts.txt",
                      col_names = FALSE,
                      col_types = paste0(rep("c", 426),
                                         collapse = "")) %>% 
  pivot_longer(., cols = everything()) %>% 
  separate(., value, c("year", "match_total", "page_total", "volume_total")) %>% 
  select(., -name) %>% 
  mutate_if(., is.character, as.numeric) %>% # "mutate_if" is superseded, but it works for now.
  na.omit(.) %>% # There is a final row in the dataset that is all NA values. Best to remove it.
  filter(., year >= 1800 & year <= 2000) # We don't use the other years, so we can prune the data.

mydata2 <- read_table("/Users/kaanaksoy/Desktop/UWM/Fall 2022/POL SCI 935/Assignment 2/all1gram",
                      col_names = c("ngram", "year", "total", "volume")) %>%
  filter(., year >= 1800 & year <= 2000) # Again, we can prune the years as before.

# You don't have to name the variables as I have, obviously. However, I have done so in order to be able to easily parse the different variables in the next stage, as it will involve merging the two datasets. Being able to distinguish between the variables of the two different datasets made my job easier. Your mileage may vary.
```


```{r, warning=FALSE}

data <- as.data.frame(matrix(ncol=1, nrow=209))
data$V1 <- seq(from=1800, to=2008)
names(data)[names(data)=="V1"] <- "Year"
search_terms <- c("1883", "1910", "1950")

for(i in 1:length(search_terms))
  {term <- search_terms[i]
  temp <- ngram(term, corpus = "en-2012", year_start = 1800,
                smoothing = 0, count = T, case_ins = FALSE)
  data <- merge(data, temp[,c("Year", "Count")], 
                by ="Year", all.x=TRUE)
  colname <- paste(term, sep="")
  names(data)[names(data)=="Count"] <- colname
  rm(temp)}

names(data)[names(data)=="1883"] <- "total-term1883"
names(data)[names(data)=="1910"] <- "total-term1910"
names(data)[names(data)=="1950"] <- "total-term1950"

##Total count data
counts_file <- file.path("Downloads", "googlebooks-eng-all-totalcounts-20120701.txt")

if (!dir.exists("data")) {
  dir.create("data")
}

if (!file.exists(counts_file)) {
  download.file(
    "http://storage.googleapis.com/books/ngrams/books/googlebooks-eng-all-totalcounts-20120701.txt",
    counts_file
  )
}

total_counts_temp <- t(
  read.table(
    counts_file, 
    header = FALSE
  )
)

total_counts_char <- do.call(
  rbind, 
  strsplit(total_counts_temp, ",")
)
total_counts <- apply(total_counts_char, 2, as.numeric)
colnames(total_counts) <- c("Year", "match_count", "page_count", "volume_count")

data2 <- as.tibble(total_counts)

df <- subset(data2, data2$Year >= "1800" & data2$Year <= "2008")
dataframe <- merge(data, df, by = "Year")


```

b) Recreate the main part of figure 3a of Michel et al. (2011). To recreate this figure, you will need two files: the one you downloaded in part (a) and the "total counts" file, which you can use to convert the raw counts into proportions. Note that the total counts file has a structure that may make it a bit hard to read in. Does version 2 of the NGram data produce similar results to those presented in Michel et al. (2011), which are based on version 1 data?

```{r}
## Alternative method with ggplot2 and RColorBrewer. RColorBrewer is only necessary for changing the colour palette, so if you're happy with ggplot2's native colour scheme, you can leave it out.

# Creating the necessary variables to replicate the figure.
df1 <- mydata2 %>% 
  filter(., ngram == c("1883", "1910", "1950")) %>% 
  left_join(., mydata1, by = "year") %>% 
  # This is in order to ensure that the total counts of all ngrams for all years are in the same tibble. Otherwise, calculating frequency (done in the next command) becomes unnecessarily complicated.
  mutate(., freq = (total/match_total)*100)

# Replicating the figure (Fig. 3a).
library(RColorBrewer)

ggplot(df1, aes(x = year, y = freq, colour = ngram)) + # Colour needs to be provided at this stage.
  geom_line(size = 0.75) + # Aesthetic preference, making lines a little thicker.
  scale_x_continuous(breaks = seq(from = 1875, to = 2000, by = 25),
                     limits = c(1860, 2000)) +
  scale_colour_brewer(palette = "Dark2") + # Adding a better colour scheme with "RColorBrewer" package.
  labs(x = "Year", y = "Frequency", colour = "") + # Setting axis titles and removing legend title.
  theme_minimal() # Aesthetic preference, not necessary.
```


```{r, warning=false}

dataframe$freq1833 <- dataframe$`total-term1883` / dataframe$match_count* 10000

dataframe$freq1910 <- dataframe$`total-term1910` / dataframe$match_count* 10000

dataframe$freq1950 <- dataframe$`total-term1950` / dataframe$match_count* 10000

##Plot based on frequency

plot(c(1875,2000), c(0,1.5), type="n", main="Figure 3a (main)",ylab="Relative Frequency", xlab="Year")
lines(dataframe$Year,dataframe$freq1833,col="blue", lwd=2)
lines(dataframe$Year,dataframe$freq1910,col="green", lwd=2)
lines(dataframe$Year,dataframe$freq1950,col="red", lwd=2)
legend(1880,1.5,legend = c("\"1883\"","\"1910\"","\"1950\""), col=c("blue","green","red"),cex=.8, lwd=2, y.intersp=1)


```

c) Now check your graph against the graph created by the NGram Viewer.

[Your interpretation comes here.]

d) Recreate figure 3a (main figure), but change the y-axis to be the raw mention count (not the rate of mentions).

```{r}
## Alternative method with ggplot2.

ggplot(df1, aes(x = year, y = total, colour = ngram)) + # Colour needs to be provided at this stage.
  geom_line(size = 0.75) + # Aesthetic preference, making lines a little thicker.
  scale_y_continuous(labels = scales::comma, # Removing scientific notation.
                     limits = c(0, 300000)) + # Increasing upper limit for interpretability.
  scale_x_continuous(breaks = seq(from = 1875, to = 2000, by = 25),
                     limits = c(1860, 2000)) +
  scale_colour_brewer(palette = "Dark2") + # Adding a better colour scheme with "RColorBrewer" package.
  labs(x = "Year", y = "Frequency", colour = "") + # Setting axis titles and removing legend title.
  theme_minimal() # Aesthetic preference, not necessary.
```


```{r, warning=FALSE}

plot(c(1875,2000),c(0,max(dataframe$`total-term1950`)), type="n", main="Figure 3a, absolute counts",ylab="Count", xlab="Year")
lines(dataframe$Year,dataframe$`total-term1883`,col="blue", lwd=2)
lines(dataframe$Year,dataframe$`total-term1910`,col="green", lwd=2)
lines(dataframe$Year,dataframe$`total-term1950`,col="red", lwd=2)
legend(1890,250000,legend = c("\"1883\"","\"1910\"","\"1950\""), col=c("blue","green","red"),cex=.8, lwd=2, y.intersp=1)

```

e) Does the difference between (b) and (d) lead you to reevaluate any of the results of Michel et al. (2011). Why or why not?

[Your interpretation comes here.]

f) Now, using the proportion of mentions, replicate the inset of figure 3a. That is, for each year between 1875 and 1975, calculate the half-life of that year. The half-life is defined to be the number of years that pass before the proportion of mentions reaches half its peak value. Note that Michel et al. (2011) do something more complicated to estimate the half- life---see section III.6 of their Supporting Online Information---but they claim that both approaches produce similar results. Does version 2 of the NGram data produce similar results to those presented in Michel et al. (2011), which are based on version 1 data? (Hint: Don’t be surprised if it doesn’t.)



g) Were there any years that were outliers, such as years that were forgotten particularly quickly or particularly slowly? Briefly speculate about possible reasons for that pattern and explain how you identified the outliers. (Hint: Check Healy 2018, ch.5 for tips to label outliers.)



h) Now replicate this result for version 2 of the NGrams data in one of the following languages: Chinese, French, German, Hebrew, Italian, Russian or Spanish.

```{r, warning=FALSE}
# Language codes:
# Chinese: "zh-CN" -- Unsure about this, doesn't seem to work. "zh" doesn't work.
# French: "fr"
# German: "de"
# Hebrew: "iw"
# Italian: "it"
# Russian: "ru"
# Spanish: "es"
```

i) Comparing between both languages, were there any years that were outliers, such as years that were forgotten particularly quickly or particularly slowly? Briefly speculate about possible reasons for that pattern.


