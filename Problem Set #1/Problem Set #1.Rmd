---
title: 'Problem Set #1'
author: ''
date: "2022-09-23"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

*Note:* Please prepare your answers using Rmarkdown and submit a pdf via Canvas. Each submission has to include all code and R output used to answer the questions. I encourage you to work on the assignments together, but each of you have to type up their responses individually. Identical submissions will not be accepted. Late submissions may only receive partial credit at my discretion.


Penney (2016) explored whether the widespread publicity about NSA/PRISM surveillance (i.e., the Snowden revelations) in June 2013 was associated with a sharp and sudden decrease in traffic to Wikipedia articles on topics that raise privacy concerns. If so, this change in behavior would be consistent with a chilling effect resulting from mass surveillance. The approach of Penney (2016) is sometimes called an interrupted time series design, and it is related to the approaches described in section 2.4.3. of Salganik (2019).

To choose the topic keywords, Penney referred to the list used by the US Department of Homeland Security for tracking and monitoring social media. The DHS list categorizes certain search terms into a range of issues, i.e., “Health Concern,” “Infrastructure Security,” and “Terrorism.” For the study group, Penney used the 48 keywords related to “Terrorism” (see appendix table 8). He then aggregated Wikipedia article view counts on a monthly basis for the corresponding 48 Wikipedia articles over a 32-month period from the beginning of January 2012 to the end of August 2014. To strengthen his argument, he also created several comparison groups by tracking article views on other topics. Now, you are going to replicate Penney (2016). All the raw data that you will need for this activity is available from Wikipedia (https://dumps.wikimedia.org/other/pagecounts-raw/ (Links to an external site.)). Or you can get it from the R-package wikipediatrend. When you write up your responses, please note which data source you used. This activity will give you practice in data wrangling and thinking about discovering natural experiments in big data sources. It will also get you up and running with a potentially interesting data source for future projects.

```{r,message=FALSE}

library(lubridate)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(wikipediatrend)
library(rvest) 

dat <- wp_trend("Al-Qaeda", lang = "en", from = "2012-01-01", to = "2014-12-12")


```

```{r, message=FALSE}
# Alternative way to go about things using the "rvest" package and with a loop.

library(tidyverse)
library(rvest)

# Doing it for a single file.

file <- read_html("https://dumps.wikimedia.org/other/pagecounts-raw/2012/2012-01/") %>%
  html_nodes("li") %>%
  html_node("a") %>% 
  html_attr("href") %>% 
  str_subset("pagecounts")

temp <- tempfile()
# The next line of code will download from the internet. May take some time depending on your download speed.
download.file(paste0("https://dumps.wikimedia.org/other/pagecounts-raw/2012/2012-01/", file[1]), temp)
gzfile(temp, 'rt')
mydata1 <- read.table(temp, sep = " ") %>% 
  as_tibble() %>% 
  filter(V1 == "en",
         V2 == "Al-Qaeda")

# What the above did was to isolate the page counts of the page "Al-Qaeda" in the English language. If you check (run "length(file)" to do so), there are 744 files in the database we looked at. The previous did the process only with one. Therefore, the question becomes a question of doing it for the remaining 743 files without us having to manually put them all together.

for (i in 2:length(file)) {
  
  # Doing the same deal, but within the loop.
  download.file(paste0("https://dumps.wikimedia.org/other/pagecounts-raw/2012/2012-01/", file[i]), temp)
  gzfile(temp, 'rt')
  df <- read.table(temp, sep = "", fill = T) %>% 
    as_tibble() %>% 
    filter(V1 == "en",
           V2 == "Al-Qaeda")
  
  # Adding the created 1x4 data frame to the end of the main data.
  mydata1 <- rbind(mydata1, df)
  
}
```


a) Read Penney (2016) and replicate his figure 2, which shows the page views for “Terrorism”-related pages before and after the Snowden revelations. Interpret the findings.

```{r}

data <- dat %>% 
  dplyr::filter(date < "2012-01-01" | date > "2013-06-01" ) %>% 
  arrange(date)

tibble::tibble(data)

# So I am able to make a general plot; but the x-axis in the article 
# has the count of months which I am unable to figure out atm
# once the dates are in a count order; add 'geom_vline(xintercept = )' 
# I also just noticed my y 'views' total does not much up in either data frame

data %>% 
  ggplot(aes(date, views)) + geom_point() + geom_smooth(method = "loess") + 
  labs(title = "", 
       x = "Time (Months)", 
       y = "Total Views")


```


b) Next, replicate figure 4A, which compares the study group (“Terrorism”- related articles) with a comparator group using keywords categorized under “DHS & Other Agencies” from the DHS list (see appendix table 10 and footnote 139). Interpret the findings.

```{r}
# When I run the code below the only 'article' type is al-qaeda' 
# I dont see a 'DHS & Other Agencies' in the raw data frame 

unique(dat$article)




```


c) In part (b) you compared the study group with one comparator group. Penney also compared with two other comparator groups: “Infrastructure Security”–related articles (appendix table 11) and popular Wikipedia pages (appendix table 12). Come up with an alternative comparator group, and test whether the findings from part (b) are sensitive to your choice of comparator group. Which choice makes most sense? Why?









